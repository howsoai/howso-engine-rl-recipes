Title: Search | arXiv e-print repository

URL Source: https://arxiv.org/search/?query=cartpole&searchtype=all&abstracts=show&order=-announced_date_first&size=50

Markdown Content:
1.   Hybrid Quantum-Classical Policy Gradient for Adaptive Control of Cyber-Physical Systems: A Comparative Study of VQC vs. MLP

Abstract:  …study employed a multilayer perceptron (MLP) agent as a classical baseline and a parameterized variational quantum circuit (VQC) as a quantum counterpart, both trained on the CartPole-v1 environment over 500 episodes. Empirical results demonstrated that the classical MLP achieved near-optimal policy convergence with a mean return of 498.7 +/- 3.2, maintainin… [▽ More](https://arxiv.org/search/?query=cartpole&searchtype=all&abstracts=show&order=-announced_date_first&size=50)

Submitted 7 October, 2025; originally announced October 2025.

2.   A Control-Barrier-Function-Based Algorithm for Policy Adaptation in Reinforcement Learning

Authors:[Wenjian Hao](https://arxiv.org/search/?searchtype=author&query=Hao%2C+W), [Zehui Lu](https://arxiv.org/search/?searchtype=author&query=Lu%2C+Z), [Nicolas Miguel](https://arxiv.org/search/?searchtype=author&query=Miguel%2C+N), [Shaoshuai Mou](https://arxiv.org/search/?searchtype=author&query=Mou%2C+S)

Abstract:  …property of control barrier functions to guarantee constraint satisfaction. The effectiveness of the proposed method is demonstrated through numerical experiments on the Cartpole and Lunar Lander benchmarks from OpenAI Gym, as well as a quadruped robot, thereby illustrating both its practicality and potential for real-world policy adaptation. [▽ More](https://arxiv.org/search/?query=cartpole&searchtype=all&abstracts=show&order=-announced_date_first&size=50)

Submitted 3 October, 2025; originally announced October 2025.

3.   Directed-MAML: Meta Reinforcement Learning Algorithm with Task-directed Approximation

Authors:[Yang Zhang](https://arxiv.org/search/?searchtype=author&query=Zhang%2C+Y), [Huiwen Yan](https://arxiv.org/search/?searchtype=author&query=Yan%2C+H), [Mushuang Liu](https://arxiv.org/search/?searchtype=author&query=Liu%2C+M)

Abstract:  …computational cost. Experimental results demonstrate that Directed-MAML surpasses MAML-based baselines in computational efficiency and convergence speed in the scenarios of CartPole-v1, LunarLander-v2 and two-vehicle intersection crossing. Furthermore, we show that task-directed approximation can be effectively integrated into other meta-learning algorithms,… [▽ More](https://arxiv.org/search/?query=cartpole&searchtype=all&abstracts=show&order=-announced_date_first&size=50)

Submitted 30 September, 2025; originally announced October 2025.

4.   Sampling Complexity of TD and PPO in RKHS

Authors:[Lu Zou](https://arxiv.org/search/?searchtype=author&query=Zou%2C+L), [Wendi Ren](https://arxiv.org/search/?searchtype=author&query=Ren%2C+W), [Weizhong Zhang](https://arxiv.org/search/?searchtype=author&query=Zhang%2C+W), [Liang Ding](https://arxiv.org/search/?searchtype=author&query=Ding%2C+L), [Shuang Li](https://arxiv.org/search/?searchtype=author&query=Li%2C+S)

Abstract:  …convergence rate for stochastic optimization. Empirically, the theory-aligned schedule improves stability and sample efficiency on common control tasks (e.g., CartPole, Acrobot), while our TD-based critic attains favorable throughput versus a GAE baseline. Altogether, our results place PPO on a firmer theoretical footing beyond finite-dimensional assumption… [▽ More](https://arxiv.org/search/?query=cartpole&searchtype=all&abstracts=show&order=-announced_date_first&size=50)

Submitted 29 September, 2025; originally announced September 2025.

5.   Gradient Free Deep Reinforcement Learning With TabPFN

Authors:[David Schiff](https://arxiv.org/search/?searchtype=author&query=Schiff%2C+D), [Ofir Lindenbaum](https://arxiv.org/search/?searchtype=author&query=Lindenbaum%2C+O), [Yonathan Efroni](https://arxiv.org/search/?searchtype=author&query=Efroni%2C+Y)

Abstract:  …that retains only the top 5% of trajectories. Empirical evaluations on the Gymnasium classic control suite demonstrate that TabPFN RL matches or surpasses Deep Q Network on CartPole v1, MountainCar v0, and Acrobot v1, without applying gradient descent or any extensive hyperparameter tuning. We discuss the theoretical aspects of how bootstrapped targets and n… [▽ More](https://arxiv.org/search/?query=cartpole&searchtype=all&abstracts=show&order=-announced_date_first&size=50)

Submitted 14 September, 2025; originally announced September 2025.

6.   Poke and Strike: Learning Task-Informed Exploration Policies

Authors:[Marina Y. Aoyama](https://arxiv.org/search/?searchtype=author&query=Aoyama%2C+M+Y), [Joao Moura](https://arxiv.org/search/?searchtype=author&query=Moura%2C+J), [Juan Del Aguila Ferrandis](https://arxiv.org/search/?searchtype=author&query=Ferrandis%2C+J+D+A), [Sethu Vijayakumar](https://arxiv.org/search/?searchtype=author&query=Vijayakumar%2C+S)

Abstract:  …at test time. Additionally, we demonstrate that our task-informed rewards capture the relative importance of physical properties in both the striking task and the classical CartPole example. Finally, we validate our approach by demonstrating its ability to identify object properties and adjust task execution in a physical setup using the KUKA iiwa robot arm. [▽ More](https://arxiv.org/search/?query=cartpole&searchtype=all&abstracts=show&order=-announced_date_first&size=50)

Submitted 29 August, 2025; originally announced September 2025.

7.   Spiking Decision Transformers: Local Plasticity, Phase-Coding, and Dendritic Routing for Low-Power Sequence Control

Authors:[Vishal Pandey](https://arxiv.org/search/?searchtype=author&query=Pandey%2C+V), [Debasmita Biswas](https://arxiv.org/search/?searchtype=author&query=Biswas%2C+D)

Abstract:  …encodings, and a lightweight dendritic routing module. Our implementation matches or exceeds standard Decision Transformer performance on classic control benchmarks (CartPole-v1, MountainCar-v0, Acrobot-v1, Pendulum-v1) while emitting fewer than ten spikes per decision, an energy proxy suggesting over four orders-of-magnitude reduction in per inference energ… [▽ More](https://arxiv.org/search/?query=cartpole&searchtype=all&abstracts=show&order=-announced_date_first&size=50)

Submitted 29 August, 2025; originally announced August 2025.

8.   Beyond ReLU: Chebyshev-DQN for Enhanced Deep Q-Networks

Authors:[Saman Yazdannik](https://arxiv.org/search/?searchtype=author&query=Yazdannik%2C+S), [Morteza Tayefi](https://arxiv.org/search/?searchtype=author&query=Tayefi%2C+M), [Shamim Sanisales](https://arxiv.org/search/?searchtype=author&query=Sanisales%2C+S)

Abstract:  …properties of Chebyshev polynomials, we hypothesize that the Ch-DQN can learn more efficiently and achieve higher performance. We evaluate our proposed model on the CartPole-v1 benchmark and compare it against a standard DQN with a comparable number of parameters. Our results demonstrate that the Ch-DQN with a moderate polynomial degree (N=4) achieves signif… [▽ More](https://arxiv.org/search/?query=cartpole&searchtype=all&abstracts=show&order=-announced_date_first&size=50)

Submitted 20 August, 2025; originally announced August 2025.

9.   Nonlinear Photonic Neuromorphic Chips for Spiking Reinforcement Learning

Authors:[Shuiying Xiang](https://arxiv.org/search/?searchtype=author&query=Xiang%2C+S), [Yonghang Chen](https://arxiv.org/search/?searchtype=author&query=Chen%2C+Y), [Haowen Zhao](https://arxiv.org/search/?searchtype=author&query=Zhao%2C+H), [Shangxuan Shi](https://arxiv.org/search/?searchtype=author&query=Shi%2C+S), [Xintao Zeng](https://arxiv.org/search/?searchtype=author&query=Zeng%2C+X), [Yahui Zhang](https://arxiv.org/search/?searchtype=author&query=Zhang%2C+Y), [Xingxing Guo](https://arxiv.org/search/?searchtype=author&query=Guo%2C+X), [Yanan Han](https://arxiv.org/search/?searchtype=author&query=Han%2C+Y), [Ye Tian](https://arxiv.org/search/?searchtype=author&query=Tian%2C+Y), [Yuechun Shi](https://arxiv.org/search/?searchtype=author&query=Shi%2C+Y), [Yue Hao](https://arxiv.org/search/?searchtype=author&query=Hao%2C+Y)

Abstract:  …photonic nonlinear computation: 987.65 GOPS/W) and low-latency (320 ps) end-to-end deployment of an entire layer of photonic spiking RL. Two RL benchmarks include the discrete CartPole task and the continuous Pendulum tasks are demonstrated experimentally based on spiking proximal policy optimization algorithm. The hardware-software collaborative computing r… [▽ More](https://arxiv.org/search/?query=cartpole&searchtype=all&abstracts=show&order=-announced_date_first&size=50)

Submitted 9 August, 2025; originally announced August 2025.

10.   Physics-informed approach for exploratory Hamilton--Jacobi--Bellman equations via policy iterations

Authors:[Yeongjong Kim](https://arxiv.org/search/?searchtype=author&query=Kim%2C+Y), [Namkyeong Cho](https://arxiv.org/search/?searchtype=author&query=Cho%2C+N), [Minseok Kim](https://arxiv.org/search/?searchtype=author&query=Kim%2C+M), [Yeoneung Kim](https://arxiv.org/search/?searchtype=author&query=Kim%2C+Y)

Abstract:  …is validated with a range of challenging control tasks, including high-dimensional linear-quadratic regulation in 5D and 10D, as well as nonlinear systems such as pendulum and cartpole problems. Numerical results confirm the scalability, accuracy, and robustness of our approach across both linear and nonlinear benchmarks. [▽ More](https://arxiv.org/search/?query=cartpole&searchtype=all&abstracts=show&order=-announced_date_first&size=50)

Submitted 3 August, 2025; originally announced August 2025.

11.   Neural Policy Iteration for Stochastic Optimal Control: A Physics-Informed Approach

Authors:[Yeongjong Kim](https://arxiv.org/search/?searchtype=author&query=Kim%2C+Y), [Yeoneung Kim](https://arxiv.org/search/?searchtype=author&query=Kim%2C+Y), [Minseok Kim](https://arxiv.org/search/?searchtype=author&query=Kim%2C+M), [Namkyeong Cho](https://arxiv.org/search/?searchtype=author&query=Cho%2C+N)

Abstract:  …convergence guarantees of classical policy iteration under mild conditions. We demonstrate the effectiveness of our method on several benchmark problems, including stochastic cartpole, pendulum problems and high-dimensional linear quadratic regulation (LQR) problems in up to 10D. [▽ More](https://arxiv.org/search/?query=cartpole&searchtype=all&abstracts=show&order=-announced_date_first&size=50)

Submitted 3 August, 2025; originally announced August 2025.

12.   MorphoNAS: Embryogenic Neural Architecture Search Through Morphogen-Guided Development

Authors:[Mykola Glybovets](https://arxiv.org/search/?searchtype=author&query=Glybovets%2C+M), [Sergii Medvid](https://arxiv.org/search/?searchtype=author&query=Medvid%2C+S)

Abstract:  …in which MorphoNAS system was able to find fully successful genomes able to generate predefined random graph configurations (8-31 nodes); and functional performance on the CartPole control task achieving low complexity 6-7 neuron solutions when target network size minimization evolutionary pressure was applied. The evolutionary process successfully balanced… [▽ More](https://arxiv.org/search/?query=cartpole&searchtype=all&abstracts=show&order=-announced_date_first&size=50)

Submitted 18 July, 2025; originally announced July 2025.

13.   Learning to Control Dynamical Agents via Spiking Neural Networks and Metropolis-Hastings Sampling

Authors:[Ali Safa](https://arxiv.org/search/?searchtype=author&query=Safa%2C+A), [Farida Mohsen](https://arxiv.org/search/?searchtype=author&query=Mohsen%2C+F), [Ali Al-Zawqari](https://arxiv.org/search/?searchtype=author&query=Al-Zawqari%2C+A)

Abstract:  …the limitations of backpropagation while enabling direct optimization on neuromorphic platforms. We evaluated this framework on two standard control benchmarks: AcroBot and CartPole. The results demonstrate that our MH-based approach outperforms conventional Deep Q-Learning (DQL) baselines and prior SNN-based RL approaches in terms of maximizing the accumula… [▽ More](https://arxiv.org/search/?query=cartpole&searchtype=all&abstracts=show&order=-announced_date_first&size=50)

Submitted 13 July, 2025; originally announced July 2025.

14.   Benchmarking Spiking Neurons for Linear Quadratic Regulator Control of Multi-linked Pole on a Cart: from Single Neuron to Ensemble

Authors:[Shreyan Banerjee](https://arxiv.org/search/?searchtype=author&query=Banerjee%2C+S), [Luna Gava](https://arxiv.org/search/?searchtype=author&query=Gava%2C+L), [Aasifa Rounak](https://arxiv.org/search/?searchtype=author&query=Rounak%2C+A), [Vikram Pakrashi](https://arxiv.org/search/?searchtype=author&query=Pakrashi%2C+V)

Abstract:  …application, thus increasing the power consumption and on-board resource utilization. A transition from two neurons to a population of neurons for the LQR control of a cartpole is shown in this work. The near-linear behavior of a Leaky-Integrate-and-Fire neuron can be exploited to achieve the Linear Quadratic Regulator (LQR) control of a… [▽ More](https://arxiv.org/search/?query=cartpole&searchtype=all&abstracts=show&order=-announced_date_first&size=50)

Submitted 4 July, 2025; originally announced July 2025.

15.   On-Policy Optimization of ANFIS Policies Using Proximal Policy Optimization

Authors:[Kaaustaaub Shankar](https://arxiv.org/search/?searchtype=author&query=Shankar%2C+K), [Wilhelm Louw](https://arxiv.org/search/?searchtype=author&query=Louw%2C+W), [Kelly Cohen](https://arxiv.org/search/?searchtype=author&query=Cohen%2C+K)

Abstract:  …that used Deep Q-Networks (DQN) with Adaptive Neuro-Fuzzy Inference Systems (ANFIS), our PPO-based framework leverages a stable on-policy actor-critic setup. Evaluated on the CartPole-v1 environment across multiple seeds, PPO-trained fuzzy agents consistently achieved the maximum return of 500 with zero variance after 20000 updates, outperforming ANFIS-DQN b… [▽ More](https://arxiv.org/search/?query=cartpole&searchtype=all&abstracts=show&order=-announced_date_first&size=50)

Submitted 3 July, 2025; v1 submitted 22 June, 2025; originally announced July 2025.

16.   SafeRL-Lite: A Lightweight, Explainable, and Constrained Reinforcement Learning Library

Authors:[Satyam Mishra](https://arxiv.org/search/?searchtype=author&query=Mishra%2C+S), [Phung Thao Vi](https://arxiv.org/search/?searchtype=author&query=Vi%2C+P+T), [Shivam Mishra](https://arxiv.org/search/?searchtype=author&query=Mishra%2C+S), [Vishwanath Bijalwan](https://arxiv.org/search/?searchtype=author&query=Bijalwan%2C+V), [Vijay Bhaskar Semwal](https://arxiv.org/search/?searchtype=author&query=Semwal%2C+V+B), [Abdul Manan Khan](https://arxiv.org/search/?searchtype=author&query=Khan%2C+A+M)

Abstract:  …is lightweight, extensible, and installable via pip, and includes built-in metrics for constraint violations. We demonstrate its effectiveness on constrained variants of CartPole and provide visualizations that reveal both policy logic and safety adherence. The full codebase is available at: https://github.com/satyamcser/saferl-lite. [▽ More](https://arxiv.org/search/?query=cartpole&searchtype=all&abstracts=show&order=-announced_date_first&size=50)

Submitted 17 June, 2025; originally announced June 2025.

17.   Heterogeneous Federated Reinforcement Learning Using Wasserstein Barycenters

Authors:[Luiz Pereira](https://arxiv.org/search/?searchtype=author&query=Pereira%2C+L), [M. Hadi Amini](https://arxiv.org/search/?searchtype=author&query=Amini%2C+M+H)

Abstract:  …the processes created in the first part of the paper to develop an algorithm to tackle Heterogeneous Federated Reinforcement Learning (HFRL). Our test experiment is the CartPole toy problem, where we vary the lengths of the poles to create heterogeneous environments. We train a deep Q-Network (DQN) in each environment to learn to control each cart, while occ… [▽ More](https://arxiv.org/search/?query=cartpole&searchtype=all&abstracts=show&order=-announced_date_first&size=50)

Submitted 18 June, 2025; originally announced June 2025.

18.   Unsupervised Meta-Testing with Conditional Neural Processes for Hybrid Meta-Reinforcement Learning

Authors:[Suzan Ece Ada](https://arxiv.org/search/?searchtype=author&query=Ada%2C+S+E), [Emre Ugur](https://arxiv.org/search/?searchtype=author&query=Ugur%2C+E)

Abstract:  …can adapt to an unseen test task using significantly fewer samples during meta-testing than the baselines in 2D-Point Agent and continuous control meta-RL benchmarks, namely, cartpole with unknown angle sensor bias, walker agent with randomized dynamics parameters. [▽ More](https://arxiv.org/search/?query=cartpole&searchtype=all&abstracts=show&order=-announced_date_first&size=50)

Submitted 4 June, 2025; originally announced June 2025.

19.   BASIL: Best-Action Symbolic Interpretable Learning for Evolving Compact RL Policies

Authors:[Kourosh Shahnazari](https://arxiv.org/search/?searchtype=author&query=Shahnazari%2C+K), [Seyed Moein Ayyoubzadeh](https://arxiv.org/search/?searchtype=author&query=Ayyoubzadeh%2C+S+M), [Mohammadali Keshtparvar](https://arxiv.org/search/?searchtype=author&query=Keshtparvar%2C+M)

Abstract:  …supports the use of exact constraints for rule count and system adaptability for balancing transparency with expressiveness. Empirical comparisons with three benchmark tasks CartPole-v1, MountainCar-v0, and Acrobot-v1 show that BASIL consistently synthesizes interpretable controllers with compact representations comparable to deep reinforcement learning base… [▽ More](https://arxiv.org/search/?query=cartpole&searchtype=all&abstracts=show&order=-announced_date_first&size=50)

Submitted 11 June, 2025; v1 submitted 30 May, 2025; originally announced June 2025.

20.   Latent Representations for Control Design with Provable Stability and Safety Guarantees

Authors:[Paul Lutkus](https://arxiv.org/search/?searchtype=author&query=Lutkus%2C+P), [Kaiyuan Wang](https://arxiv.org/search/?searchtype=author&query=Wang%2C+K), [Lars Lindemann](https://arxiv.org/search/?searchtype=author&query=Lindemann%2C+L), [Stephen Tu](https://arxiv.org/search/?searchtype=author&query=Tu%2C+S)

Abstract:  …dynamics reconstruction that are directly related to control design. We conclude by demonstrating the applicability of our theory to two case studies: (1) stabilization of a cartpole system, and (2) collision avoidance for a two vehicle system. [▽ More](https://arxiv.org/search/?query=cartpole&searchtype=all&abstracts=show&order=-announced_date_first&size=50)

Submitted 29 May, 2025; originally announced May 2025.

21.   Bellman operator convergence enhancements in reinforcement learning algorithms

Authors:[David Krame Kadurha](https://arxiv.org/search/?searchtype=author&query=Kadurha%2C+D+K), [Domini Jocema Leko Moutouo](https://arxiv.org/search/?searchtype=author&query=Moutouo%2C+D+J+L), [Yae Ulrich Gaba](https://arxiv.org/search/?searchtype=author&query=Gaba%2C+Y+U)

Abstract:  …alternative formulations of Bellman operators and demonstrate their impact on improving convergence rates and performance in standard RL environments such as MountainCar, CartPole, and Acrobot. Our findings highlight how a deeper mathematical understanding of RL can lead to more effective algorithms for decision-making problems. [▽ More](https://arxiv.org/search/?query=cartpole&searchtype=all&abstracts=show&order=-announced_date_first&size=50)

Submitted 20 May, 2025; originally announced May 2025.

22.   Improving the Data-efficiency of Reinforcement Learning by Warm-starting with LLM

Authors:[Thang Duong](https://arxiv.org/search/?searchtype=author&query=Duong%2C+T), [Minglai Yang](https://arxiv.org/search/?searchtype=author&query=Yang%2C+M), [Chicheng Zhang](https://arxiv.org/search/?searchtype=author&query=Zhang%2C+C)

Abstract:  …LORO, can both converge to an optimal policy and have a high sample efficiency thanks to the LLM's good starting policy. On multiple OpenAI Gym environments, such as CartPole and Pendulum, we empirically demonstrate that LORO outperforms baseline algorithms such as pure LLM-based policies, pure RL, and a naive combination of the two, achieving up to… [▽ More](https://arxiv.org/search/?query=cartpole&searchtype=all&abstracts=show&order=-announced_date_first&size=50)

Submitted 16 May, 2025; originally announced May 2025.

23.   Improving Controller Generalization with Dimensionless Markov Decision Processes

Authors:[Valentin Charvet](https://arxiv.org/search/?searchtype=author&query=Charvet%2C+V), [Sebastian Stein](https://arxiv.org/search/?searchtype=author&query=Stein%2C+S), [Roderick Murray-Smith](https://arxiv.org/search/?searchtype=author&query=Murray-Smith%2C+R)

Abstract:  …approach and apply it to a model-based policy search algorithm using Gaussian Process models. We demonstrate the applicability of our method on simulated actuated pendulum and cartpole systems, where policies trained on a single environment are robust to shifts in the distribution of the context. [▽ More](https://arxiv.org/search/?query=cartpole&searchtype=all&abstracts=show&order=-announced_date_first&size=50)

Submitted 14 April, 2025; originally announced April 2025.

24.   Learning Multi-Robot Coordination through Locality-Based Factorized Multi-Agent Actor-Critic Algorithm

Authors:[Chak Lam Shek](https://arxiv.org/search/?searchtype=author&query=Shek%2C+C+L), [Amrit Singh Bedi](https://arxiv.org/search/?searchtype=author&query=Bedi%2C+A+S), [Anjon Basak](https://arxiv.org/search/?searchtype=author&query=Basak%2C+A), [Ellen Novoseller](https://arxiv.org/search/?searchtype=author&query=Novoseller%2C+E), [Nick Waytowich](https://arxiv.org/search/?searchtype=author&query=Waytowich%2C+N), [Priya Narayanan](https://arxiv.org/search/?searchtype=author&query=Narayanan%2C+P), [Dinesh Manocha](https://arxiv.org/search/?searchtype=author&query=Manocha%2C+D), [Pratap Tokekar](https://arxiv.org/search/?searchtype=author&query=Tokekar%2C+P)

Abstract:  …rewards and leveraging partition-based learning to enhance training efficiency and performance. We evaluate the performance of Loc-FACMAC in three environments: Hallway, Multi-cartpole, and Bounded-Cooperative-Navigation. We explore the impact of partition sizes on the performance and compare the result with baseline MARL algorithms such as LOMAQ, FACMAC, an… [▽ More](https://arxiv.org/search/?query=cartpole&searchtype=all&abstracts=show&order=-announced_date_first&size=50)

Submitted 28 March, 2025; v1 submitted 24 March, 2025; originally announced March 2025.

25.   Actionable AI: Enabling Non Experts to Understand and Configure AI Systems

Authors:[Cécile Boulard](https://arxiv.org/search/?searchtype=author&query=Boulard%2C+C), [Sruthi Viswanathan](https://arxiv.org/search/?searchtype=author&query=Viswanathan%2C+S), [Wanda Fey](https://arxiv.org/search/?searchtype=author&query=Fey%2C+W), [Thierry Jacquin](https://arxiv.org/search/?searchtype=author&query=Jacquin%2C+T)

Abstract:  …In the absence of these elements, we discuss designing Actionable AI, which allows non-experts to configure black-box agents. In this paper, we experiment with an AI-powered cartpole game and observe 22 pairs of participants to configure it via direct manipulation. Our findings suggest that, in uncertain conditions, non-experts were able to achieve good lev… [▽ More](https://arxiv.org/search/?query=cartpole&searchtype=all&abstracts=show&order=-announced_date_first&size=50)

Submitted 9 March, 2025; originally announced March 2025.

26.   Quantifying First-Order Markov Violations in Noisy Reinforcement Learning: A Causal Discovery Approach

Authors:[Naveen Mysore](https://arxiv.org/search/?searchtype=author&query=Mysore%2C+N)

Abstract:  …Violation score (MVS). The MVS measures multi-step dependencies that emerge when noise or incomplete state information disrupts the Markov property. Classic control tasks (CartPole, Pendulum, Acrobot) serve as examples to illustrate how targeted noise and dimension omissions affect both RL performance and measured Markov consistency. Surprisingly, even sub… [▽ More](https://arxiv.org/search/?query=cartpole&searchtype=all&abstracts=show&order=-announced_date_first&size=50)

Submitted 1 June, 2025; v1 submitted 28 February, 2025; originally announced March 2025.

27.   PPO-Q: Proximal Policy Optimization with Parametrized Quantum Policies or Values

Authors:[Yu-Xin Jin](https://arxiv.org/search/?searchtype=author&query=Jin%2C+Y), [Zi-Wei Wang](https://arxiv.org/search/?searchtype=author&query=Wang%2C+Z), [Hong-Ze Xu](https://arxiv.org/search/?searchtype=author&query=Xu%2C+H), [Wei-Feng Zhuang](https://arxiv.org/search/?searchtype=author&query=Zhuang%2C+W), [Meng-Jun Hu](https://arxiv.org/search/?searchtype=author&query=Hu%2C+M), [Dong E. Liu](https://arxiv.org/search/?searchtype=author&query=Liu%2C+D+E)

Abstract:  …in the past few years. Various algorithms and techniques have been introduced, demonstrating the effectiveness of QRL in solving some popular benchmark environments such as CartPole, FrozenLake, and MountainCar. However, tackling more complex environments with continuous action spaces and high-dimensional state spaces remains challenging within the existing… [▽ More](https://arxiv.org/search/?query=cartpole&searchtype=all&abstracts=show&order=-announced_date_first&size=50)

Submitted 13 January, 2025; originally announced January 2025.

28.   Back to Base: Towards Hands-Off Learning via Safe Resets with Reach-Avoid Safety Filters

Authors:[Azra Begzadić](https://arxiv.org/search/?searchtype=author&query=Begzadi%C4%87%2C+A), [Nikhil Uday Shinde](https://arxiv.org/search/?searchtype=author&query=Shinde%2C+N+U), [Sander Tonkens](https://arxiv.org/search/?searchtype=author&query=Tonkens%2C+S), [Dylan Hirsch](https://arxiv.org/search/?searchtype=author&query=Hirsch%2C+D), [Kaleb Ugalde](https://arxiv.org/search/?searchtype=author&query=Ugalde%2C+K), [Michael C. Yip](https://arxiv.org/search/?searchtype=author&query=Yip%2C+M+C), [Jorge Cortés](https://arxiv.org/search/?searchtype=author&query=Cort%C3%A9s%2C+J), [Sylvia Herbert](https://arxiv.org/search/?searchtype=author&query=Herbert%2C+S)

Abstract:  …the feasibility of safe training for real world robots. We demonstrate our approach using a modified version of soft actor-critic to safely train a swing-up task on a modified cartpole stabilization problem. [▽ More](https://arxiv.org/search/?query=cartpole&searchtype=all&abstracts=show&order=-announced_date_first&size=50)

Submitted 2 June, 2025; v1 submitted 5 January, 2025; originally announced January 2025.

29.   Robust Neural IDA-PBC: passivity-based stabilization under approximations

Authors:[Santiago Sanchez-Escalonilla](https://arxiv.org/search/?searchtype=author&query=Sanchez-Escalonilla%2C+S), [Samuele Zoboli](https://arxiv.org/search/?searchtype=author&query=Zoboli%2C+S), [Bayu Jayawardhana](https://arxiv.org/search/?searchtype=author&query=Jayawardhana%2C+B)

Abstract:  …formulation of the system's model. Our methodology is validated with simulations on three standard benchmarks: a double pendulum, a nonlinear mass-spring-damper and a cartpole. Notably, classical IDA-PBC designs cannot be analytically derived for the latter. [▽ More](https://arxiv.org/search/?query=cartpole&searchtype=all&abstracts=show&order=-announced_date_first&size=50)

Submitted 24 September, 2024; originally announced September 2024.

30.   One-shot World Models Using a Transformer Trained on a Synthetic Prior

Authors:[Fabio Ferreira](https://arxiv.org/search/?searchtype=author&query=Ferreira%2C+F), [Moreno Schlageter](https://arxiv.org/search/?searchtype=author&query=Schlageter%2C+M), [Raghu Rajan](https://arxiv.org/search/?searchtype=author&query=Rajan%2C+R), [Andre Biedenkapp](https://arxiv.org/search/?searchtype=author&query=Biedenkapp%2C+A), [Frank Hutter](https://arxiv.org/search/?searchtype=author&query=Hutter%2C+F)

Abstract:  …predictions based on the remaining transition context. During inference time, OSWM is able to quickly adapt to the dynamics of a simple grid world, as well as the CartPole gym and a custom control environment by providing 1k transition steps as context and is then able to successfully train environment-solving agent policies. However, transferring to more co… [▽ More](https://arxiv.org/search/?query=cartpole&searchtype=all&abstracts=show&order=-announced_date_first&size=50)

Submitted 24 October, 2024; v1 submitted 21 September, 2024; originally announced September 2024.

31.   Constructive Nonlinear Control of Underactuated Systems via Zero Dynamics Policies

Authors:[William Compton](https://arxiv.org/search/?searchtype=author&query=Compton%2C+W), [Ivan Dario Jimenez Rodriguez](https://arxiv.org/search/?searchtype=author&query=Rodriguez%2C+I+D+J), [Noel Csomay-Shanklin](https://arxiv.org/search/?searchtype=author&query=Csomay-Shanklin%2C+N), [Yisong Yue](https://arxiv.org/search/?searchtype=author&query=Yue%2C+Y), [Aaron D. Ames](https://arxiv.org/search/?searchtype=author&query=Ames%2C+A+D)

Abstract:  …optimal control to obtain ZDPs with much larger regions of attraction. We demonstrate that such a paradigm can be used to stabilize the canonical underactuated system of the cartpole, and showcase an improvement over the nominal performance of LQR. [▽ More](https://arxiv.org/search/?query=cartpole&searchtype=all&abstracts=show&order=-announced_date_first&size=50)

Submitted 26 August, 2024; originally announced August 2024.

32.   Using Part-based Representations for Explainable Deep Reinforcement Learning

Authors:[Manos Kirtas](https://arxiv.org/search/?searchtype=author&query=Kirtas%2C+M), [Konstantinos Tsampazis](https://arxiv.org/search/?searchtype=author&query=Tsampazis%2C+K), [Loukia Avramelou](https://arxiv.org/search/?searchtype=author&query=Avramelou%2C+L), [Nikolaos Passalis](https://arxiv.org/search/?searchtype=author&query=Passalis%2C+N), [Anastasios Tefas](https://arxiv.org/search/?searchtype=author&query=Tefas%2C+A)

Abstract:  …training method, which can ensure better gradient flow compared to existing approaches. We demonstrate the effectiveness of the proposed approach using the well-known Cartpole benchmark. [▽ More](https://arxiv.org/search/?query=cartpole&searchtype=all&abstracts=show&order=-announced_date_first&size=50)

Submitted 22 August, 2024; v1 submitted 21 August, 2024; originally announced August 2024.

33.   Remote Tube-based MPC for Tracking Over Lossy Networks

Authors:[David Umsonst](https://arxiv.org/search/?searchtype=author&query=Umsonst%2C+D), [Fernando S. Barbosa](https://arxiv.org/search/?searchtype=author&query=Barbosa%2C+F+S)

Abstract:  …and packet losses in both directions are provided. To test the efficacy of the proposed approach, we compare it to a state-of-the-art solution in the case of controlling a cartpole system. Extensive simulations are carried out with both linearized and nonlinear system dynamics, as well as different packet loss probabilities and disturbances. The code for thi… [▽ More](https://arxiv.org/search/?query=cartpole&searchtype=all&abstracts=show&order=-announced_date_first&size=50)

Submitted 14 August, 2024; originally announced August 2024.

34.   Towards Generalizable Reinforcement Learning via Causality-Guided Self-Adaptive Representations

Authors:[Yupei Yang](https://arxiv.org/search/?searchtype=author&query=Yang%2C+Y), [Biwei Huang](https://arxiv.org/search/?searchtype=author&query=Huang%2C+B), [Fan Feng](https://arxiv.org/search/?searchtype=author&query=Feng%2C+F), [Xinyue Wang](https://arxiv.org/search/?searchtype=author&query=Wang%2C+X), [Shikui Tu](https://arxiv.org/search/?searchtype=author&query=Tu%2C+S), [Lei Xu](https://arxiv.org/search/?searchtype=author&query=Xu%2C+L)

Abstract:  …efficiently adapts to the target domains with only a few samples and outperforms state-of-the-art baselines on a wide range of scenarios, including our simulated environments, CartPole, CoinRun and Atari games. [▽ More](https://arxiv.org/search/?query=cartpole&searchtype=all&abstracts=show&order=-announced_date_first&size=50)

Submitted 5 March, 2025; v1 submitted 30 July, 2024; originally announced July 2024.

35.   Hardware Neural Control of CartPole and F1TENTH Race Car

Authors:[Marcin Paluch](https://arxiv.org/search/?searchtype=author&query=Paluch%2C+M), [Florian Bolli](https://arxiv.org/search/?searchtype=author&query=Bolli%2C+F), [Xiang Deng](https://arxiv.org/search/?searchtype=author&query=Deng%2C+X), [Antonio Rios Navarro](https://arxiv.org/search/?searchtype=author&query=Navarro%2C+A+R), [Chang Gao](https://arxiv.org/search/?searchtype=author&query=Gao%2C+C), [Tobi Delbruck](https://arxiv.org/search/?searchtype=author&query=Delbruck%2C+T)

Abstract:  …to imitate NMPC with supervised learning. We use these Neural Controllers (NCs) implemented on inexpensive embedded FPGA hardware for high frequency control on physical cartpole and F1TENTH race car. Our results show that the NCs match the control performance of the NMPCs in simulation and outperform it in reality, due to the faster control rate that is affo… [▽ More](https://arxiv.org/search/?query=cartpole&searchtype=all&abstracts=show&order=-announced_date_first&size=50)

Submitted 11 July, 2024; originally announced July 2024.

36.   From Pixels to Torques with Linear Feedback

Authors:[Jeong Hun Lee](https://arxiv.org/search/?searchtype=author&query=Lee%2C+J+H), [Sam Schoedel](https://arxiv.org/search/?searchtype=author&query=Schoedel%2C+S), [Aditya Bhardwaj](https://arxiv.org/search/?searchtype=author&query=Bhardwaj%2C+A), [Zachary Manchester](https://arxiv.org/search/?searchtype=author&query=Manchester%2C+Z)

Abstract:  …also investigate a nonlinear extension of the method via the Koopman embedding. Finally, we demonstrate the surprising effectiveness of linear pixels-to-torques policies on a cartpole system, both in simulation and on real hardware. The policy successfully executes both stabilizing and swing-up trajectory-tracking tasks using only camera feedback while subje… [▽ More](https://arxiv.org/search/?query=cartpole&searchtype=all&abstracts=show&order=-announced_date_first&size=50)

Submitted 10 December, 2024; v1 submitted 26 June, 2024; originally announced June 2024.

37.   Rewarded Region Replay (R3) for Policy Learning with Discrete Action Space

Authors:[Bangzheng Li](https://arxiv.org/search/?searchtype=author&query=Li%2C+B), [Ningshan Ma](https://arxiv.org/search/?searchtype=author&query=Ma%2C+N), [Zifan Wang](https://arxiv.org/search/?searchtype=author&query=Wang%2C+Z)

Abstract:  …also outperforms DDQN agent in DoorKeyEnv. Lastly, we adapt the idea of R3 to dense reward setting to obtain the Dense R3 algorithm (or DR3) and benchmarked it against PPO on Cartpole-V1 environment. We found that DR3 outperforms PPO significantly on this dense reward environment. Our code can be found at https://github.com/chry-santhemum/R3. [▽ More](https://arxiv.org/search/?query=cartpole&searchtype=all&abstracts=show&order=-announced_date_first&size=50)

Submitted 25 May, 2024; originally announced May 2024.

38.   Policy Gradient Methods for Risk-Sensitive Distributional Reinforcement Learning with Provable Convergence

Authors:[Minheng Xiao](https://arxiv.org/search/?searchtype=author&query=Xiao%2C+M), [Xian Yu](https://arxiv.org/search/?searchtype=author&query=Yu%2C+X), [Lei Ying](https://arxiv.org/search/?searchtype=author&query=Ying%2C+L)

Abstract:  …optimality guarantee and a finite-iteration convergence guarantee under inexact policy evaluation and gradient estimation. Through experiments on stochastic Cliffwalk and CartPole environments, we illustrate the benefits of considering a risk-sensitive setting in DRL. [▽ More](https://arxiv.org/search/?query=cartpole&searchtype=all&abstracts=show&order=-announced_date_first&size=50)

Submitted 31 January, 2025; v1 submitted 23 May, 2024; originally announced May 2024.

39.   Convexity in Optimal Control Problems

Authors:[Abhijeet](https://arxiv.org/search/?searchtype=author&query=Abhijeet), [Mohamed Naveed Gul Mohamed](https://arxiv.org/search/?searchtype=author&query=Mohamed%2C+M+N+G), [Aayushman Sharma](https://arxiv.org/search/?searchtype=author&query=Sharma%2C+A), [Suman Chakravorty](https://arxiv.org/search/?searchtype=author&query=Chakravorty%2C+S)

Abstract:  …Iterative Linear Quadratic Regulator (iLQR) and the "direct" Sequential Quadratic Programming (SQP) approach for solving the optimal control problem for the cartpole and pendulum models to validate the theoretical analysis. Results show that the ILQR always converges to the "globally" optimum solution while the SQP approach gets stuck in spur… [▽ More](https://arxiv.org/search/?query=cartpole&searchtype=all&abstracts=show&order=-announced_date_first&size=50)

Submitted 12 April, 2024; originally announced April 2024.

40.   Parameter-Adaptive Approximate MPC: Tuning Neural-Network Controllers without Retraining

Authors:[Henrik Hose](https://arxiv.org/search/?searchtype=author&query=Hose%2C+H), [Alexander Gräfe](https://arxiv.org/search/?searchtype=author&query=Gr%C3%A4fe%2C+A), [Sebastian Trimpe](https://arxiv.org/search/?searchtype=author&query=Trimpe%2C+S)

Abstract:  …model using linear predictions while still guaranteeing stability. We showcase the effectiveness of parameter-adaptive AMPC by controlling the swing-ups of two different real cartpole systems with a severely resource-constrained microcontroller (MCU). We use the same NN across both system instances that have different parameters. This work not only represent… [▽ More](https://arxiv.org/search/?query=cartpole&searchtype=all&abstracts=show&order=-announced_date_first&size=50)

Submitted 6 June, 2024; v1 submitted 8 April, 2024; originally announced April 2024.

41.   Towards Efficient Risk-Sensitive Policy Gradient: An Iteration Complexity Analysis

Authors:[Rui Liu](https://arxiv.org/search/?searchtype=author&query=Liu%2C+R), [Anish Gupta](https://arxiv.org/search/?searchtype=author&query=Gupta%2C+A), [Erfaun Noorani](https://arxiv.org/search/?searchtype=author&query=Noorani%2C+E), [Pratap Tokekar](https://arxiv.org/search/?searchtype=author&query=Tokekar%2C+P)

Abstract:  …our analysis, we empirically evaluate the learning performance and convergence efficiency of the risk-neutral and risk-sensitive REINFORCE algorithms in multiple environments: CartPole, MiniGrid, and Robot Navigation. Empirical results confirm that risk-sensitive cases can converge and stabilize faster compared to their risk-neutral counterparts. More detail… [▽ More](https://arxiv.org/search/?query=cartpole&searchtype=all&abstracts=show&order=-announced_date_first&size=50)

Submitted 29 August, 2025; v1 submitted 13 March, 2024; originally announced March 2024.

42.   Optimal Policy Sparsification and Low Rank Decomposition for Deep Reinforcement Learning

Authors:[Vikram Goddla](https://arxiv.org/search/?searchtype=author&query=Goddla%2C+V)

Abstract:  …-norm-regularization technique across five different environments (Cartpole-v1, Acrobat-v1, LunarLander-v2, SuperMarioBros-7.1.v0 and Surgical Robot Learning) using several on-policy and off-policy algorithms. We demonstrated that the $L_0$-norm-regularized DRL policy in the SuperMarioBros environment achieved 93% sparsity and gained 70% compression when sub… [▽ More](https://arxiv.org/search/?query=cartpole&searchtype=all&abstracts=show&order=-announced_date_first&size=50)

Submitted 10 March, 2024; originally announced March 2024.

43.   Barrier Functions Inspired Reward Shaping for Reinforcement Learning

Authors:[Nilaksh Nilaksh](https://arxiv.org/search/?searchtype=author&query=Nilaksh%2C+N), [Abhishek Ranjan](https://arxiv.org/search/?searchtype=author&query=Ranjan%2C+A), [Shreenabh Agrawal](https://arxiv.org/search/?searchtype=author&query=Agrawal%2C+S), [Aayush Jain](https://arxiv.org/search/?searchtype=author&query=Jain%2C+A), [Pushpak Jagtap](https://arxiv.org/search/?searchtype=author&query=Jagtap%2C+P), [Shishir Kolathaya](https://arxiv.org/search/?searchtype=author&query=Kolathaya%2C+S)

Abstract:  …and ease of implementation across various environments and tasks. To evaluate the effectiveness of the proposed reward formulations, we conduct simulation experiments on CartPole, Ant, and Humanoid environments, along with real-world deployment on the Unitree Go1 quadruped robot. Our results demonstrate that our method leads to 1.4-2.8 times faster convergen… [▽ More](https://arxiv.org/search/?query=cartpole&searchtype=all&abstracts=show&order=-announced_date_first&size=50)

Submitted 1 April, 2024; v1 submitted 3 March, 2024; originally announced March 2024.

44.   QF-tuner: Breaking Tradition in Reinforcement Learning

Authors:[Mahmood A. Jumaah](https://arxiv.org/search/?searchtype=author&query=Jumaah%2C+M+A), [Yossra H. Ali](https://arxiv.org/search/?searchtype=author&query=Ali%2C+Y+H), [Tarik A. Rashid](https://arxiv.org/search/?searchtype=author&query=Rashid%2C+T+A)

Abstract:  …value derived from observations at each iteration by executing the Q-learning algorithm. The proposed method has been evaluated using two control tasks from the OpenAI Gym: CartPole and FrozenLake. The empirical results indicate that the QF-tuner outperforms other optimization algorithms, such as particle swarm optimization (PSO), bees algorithm (BA), geneti… [▽ More](https://arxiv.org/search/?query=cartpole&searchtype=all&abstracts=show&order=-announced_date_first&size=50)

Submitted 17 March, 2025; v1 submitted 26 February, 2024; originally announced February 2024.

45.   Compressing Deep Reinforcement Learning Networks with a Dynamic Structured Pruning Method for Autonomous Driving

Authors:[Wensheng Su](https://arxiv.org/search/?searchtype=author&query=Su%2C+W), [Zhenni Li](https://arxiv.org/search/?searchtype=author&query=Li%2C+Z), [Minrui Xu](https://arxiv.org/search/?searchtype=author&query=Xu%2C+M), [Jiawen Kang](https://arxiv.org/search/?searchtype=author&query=Kang%2C+J), [Dusit Niyato](https://arxiv.org/search/?searchtype=author&query=Niyato%2C+D), [Shengli Xie](https://arxiv.org/search/?searchtype=author&query=Xie%2C+S)

Abstract:  …high and robust performance. Experimental results show that the proposed method is competitive with existing DRL pruning methods on discrete control environments (i.e., CartPole-v1 and LunarLander-v2) and MuJoCo continuous environments (i.e., Hopper-v3 and Walker2D-v3). Specifically, our method effectively compresses $93\%$ neurons and $96\%$ weights of the… [▽ More](https://arxiv.org/search/?query=cartpole&searchtype=all&abstracts=show&order=-announced_date_first&size=50)

Submitted 7 February, 2024; originally announced February 2024.

46.   Effective Communication with Dynamic Feature Compression

Authors:[Pietro Talli](https://arxiv.org/search/?searchtype=author&query=Talli%2C+P), [Francesco Pase](https://arxiv.org/search/?searchtype=author&query=Pase%2C+F), [Federico Chiariotti](https://arxiv.org/search/?searchtype=author&query=Chiariotti%2C+F), [Andrea Zanella](https://arxiv.org/search/?searchtype=author&query=Zanella%2C+A), [Michele Zorzi](https://arxiv.org/search/?searchtype=author&query=Zorzi%2C+M)

Abstract:  …adapt the quantization level, considering both the current state of the environment and the memory of past messages. We tested the proposed approach on the well-known CartPole reference control problem, obtaining a significant performance increase over traditional approaches. [▽ More](https://arxiv.org/search/?query=cartpole&searchtype=all&abstracts=show&order=-announced_date_first&size=50)

Submitted 29 January, 2024; originally announced January 2024.

47.   LangProp: A code optimization framework using Large Language Models applied to driving

Authors:[Shu Ishida](https://arxiv.org/search/?searchtype=author&query=Ishida%2C+S), [Gianluca Corrado](https://arxiv.org/search/?searchtype=author&query=Corrado%2C+G), [George Fedoseev](https://arxiv.org/search/?searchtype=author&query=Fedoseev%2C+G), [Hudson Yeo](https://arxiv.org/search/?searchtype=author&query=Yeo%2C+H), [Lloyd Russell](https://arxiv.org/search/?searchtype=author&query=Russell%2C+L), [Jamie Shotton](https://arxiv.org/search/?searchtype=author&query=Shotton%2C+J), [João F. Henriques](https://arxiv.org/search/?searchtype=author&query=Henriques%2C+J+F), [Anthony Hu](https://arxiv.org/search/?searchtype=author&query=Hu%2C+A)

Abstract:  …machine learning techniques such as imitation learning, DAgger, and reinforcement learning. We show LangProp's applicability to general domains such as Sudoku and CartPole, as well as demonstrate the first proof of concept of automated code optimization for autonomous driving in CARLA. We show that LangProp can generate interpretable and transparent poli… [▽ More](https://arxiv.org/search/?query=cartpole&searchtype=all&abstracts=show&order=-announced_date_first&size=50)

Submitted 3 May, 2024; v1 submitted 18 January, 2024; originally announced January 2024.

48.   Rapid Open-World Adaptation by Adaptation Principles Learning

Authors:[Cheng Xue](https://arxiv.org/search/?searchtype=author&query=Xue%2C+C), [Ekaterina Nikonova](https://arxiv.org/search/?searchtype=author&query=Nikonova%2C+E), [Peng Zhang](https://arxiv.org/search/?searchtype=author&query=Zhang%2C+P), [Jochen Renz](https://arxiv.org/search/?searchtype=author&query=Renz%2C+J)

Abstract:  …demonstrate the efficiency and efficacy of NAPPING, we evaluate our method on four action domains that are different in reward structures and the type of task. The domains are CartPole and MountainCar (classic control), CrossRoad (path-finding), and AngryBirds (physical reasoning). We compare NAPPING with standard online and fine-tuning DRL methods in… [▽ More](https://arxiv.org/search/?query=cartpole&searchtype=all&abstracts=show&order=-announced_date_first&size=50)

Submitted 18 December, 2023; originally announced December 2023.

49.   The Quantum Cartpole: A benchmark environment for non-linear reinforcement learning

Authors:[Kai Meinerz](https://arxiv.org/search/?searchtype=author&query=Meinerz%2C+K), [Simon Trebst](https://arxiv.org/search/?searchtype=author&query=Trebst%2C+S), [Mark Rudner](https://arxiv.org/search/?searchtype=author&query=Rudner%2C+M), [Evert van Nieuwenburg](https://arxiv.org/search/?searchtype=author&query=van+Nieuwenburg%2C+E)

Abstract:  Feedback-based control is the de-facto standard when it comes to controlling classical stochastic systems and processes. However, standard feedback-based control methods are challenged by quantum systems due to measurement induced backaction and partial observability. Here we remedy this by using weak quantum measurements and model-free reinforcement learning agents to perform quantum control. By… [▽ More](https://arxiv.org/search/?query=cartpole&searchtype=all&abstracts=show&order=-announced_date_first&size=50)

Submitted 10 March, 2024; v1 submitted 1 November, 2023; originally announced November 2023.

50.   Marginalized Importance Sampling for Off-Environment Policy Evaluation

Authors:[Pulkit Katdare](https://arxiv.org/search/?searchtype=author&query=Katdare%2C+P), [Nan Jiang](https://arxiv.org/search/?searchtype=author&query=Jiang%2C+N), [Katherine Driggs-Campbell](https://arxiv.org/search/?searchtype=author&query=Driggs-Campbell%2C+K)

Abstract:  …We analyze the sample complexity as well as error propagation of our two step-procedure. Furthermore, we empirically evaluate our approach on Sim2Sim environments such as Cartpole, Reacher, and Half-Cheetah. Our results show that our method generalizes well across a variety of Sim2Sim gap, target policies and offline data collection policies. We also demons… [▽ More](https://arxiv.org/search/?query=cartpole&searchtype=all&abstracts=show&order=-announced_date_first&size=50)

Submitted 4 October, 2023; v1 submitted 4 September, 2023; originally announced September 2023.
